---
title: "Time Series"
author: "Julius Ongteco"
date: "5/7/2021"
output:
  pdf_document: default
  html_document: default
---

## Libraries

```{r}
library(dplyr)
library(fpp2)
library(ggplot2)
library(openxlsx)
library(readxl)

```

## Initialize

```{r}

# aesthetic
theme_set(theme_classic())

# read file
e <- read_xlsx("Elec-train.xlsx")
e

# renamed columns
names(e) <- c('date', 'power', 'temp')

# power.
power <- ts(e$power, freq=96)

# temp.
temp <- ts(e$temp, freq=96)

# date.
date <- as.POSIXct(e$date,format="%m/%d/%Y %H:%M",tz=Sys.timezone())

# split data sets into one that needs to be predicted and one having values of e.
predict_e <- e[!complete.cases(e),]
values_of_e <- e[complete.cases(e),]

# predict temperature.
pred_temp <- ts(predict_e$temp, start = c(1,nrow(values_of_e)+1), end=c(1,nrow(e)), frequency=96)
```

Join our variables.

```{r}
elec <- ts.union(power, temp, pred_temp)

autoplot(power)

autoplot(temp)

autoplot(pred_temp)

```

Power graph indicates seasonality but no apparent trend. Temp graph also shows seasonality which possibly could be cyclic. let's confirm out of curiosity using an acf or pacf plot. I'm personally just enjoying this.

```{r}
ggAcf(power)

ggPacf(power)

ggAcf(temp)

ggPacf(temp)
```

autocorrelations are statistically significantly different from zero. which indicates seasonality. probably means this can possibly not be a stationary time series.


```{r}

Box.test(power,lag=10,type="Ljung-Box")

Box.test(temp,lag=10,type="Ljung-Box")

```

There are significant differences in both variables as p value is less than 0.05.

Train Test Split.

```{r}
e_train <- window(elec, start=c(1,1),end=c(1,nrow(values_of_e)-96))
e_test <- window(elec, start=c(1,nrow(values_of_e)-95),end=c(1,nrow(values_of_e)))
```


## Forecasting without Temperature

Test the Models!

Simple Exponential Smoothing.

```{r}
SES=ses(e_train[,"power"],alpha=NULL,beta=NULL,gamma=NULL)
SES_f<-predict(SES,n.ahead=96)
```

Base Holt Winters with Nonseasonal HW Smoothing.

```{r}
LES=HoltWinters(e_train[,"power"],alpha=NULL,beta=NULL,gamma=FALSE)
LES_f<-predict(LES,n.ahead=96)
```

Holt "Damped"

```{r}
LES_damped=HoltWinters(e_train[,"power"],alpha=NULL,beta=FALSE,gamma=FALSE)
LES_damped_f<-predict(LES_damped,damped=TRUE, phi=0.9, n.ahead=96)
```

Holt additive

```{r}
SES_additive=HoltWinters(e_train[,"power"],alpha=NULL,beta=NULL,gamma=NULL,seasonal = 'additive' )
SES_add<-predict(SES_additive, n.ahead=96)
```

Holt Multiplicative

```{r}
SES_multi=HoltWinters(e_train[,"power"],alpha=NULL,beta=NULL,gamma=NULL,seasonal = 'multi' )
SES_m<-predict(SES_multi, n.ahead=96)
```

Arima (Example)

```{r}
ar = Arima(e_train[,"power"], order=c(0,0,0))
ar_f = forecast(ar,h=96)
```

Auto Arima Optimal

```{r}
aa = auto.arima(e_train[,"power"], seasonal=FALSE)
aa_f = forecast(aa,h=96)
```

AA to make it work a little harder.


```{r}
aa2 <- auto.arima(e_train[,"power"], seasonal=FALSE,
  stepwise=FALSE, approximation=FALSE)
aa2_f = forecast(aa2,h=96)
```

SARIMA

```{r}
sarima = auto.arima(e_train[,"power"])
sarima_f = forecast(sarima,h=96)
```

Neural Network

```{r}
nn = nnetar(e_train[,"power"],h=96)
nn_f = forecast(nn, h=96)
```

RSME

```{r}
print(paste0("RMSE (SES): ",sqrt(mean((SES_f$mean-e_test[,"power"])^2))))
print(paste0("RMSE (HW): ",sqrt(mean((LES_f-e_test[,"power"])^2))))
print(paste0("RMSE (HW damped): ",sqrt(mean((LES_damped_f-e_test[,"power"])^2))))
print(paste0("RMSE (HW add): ",sqrt(mean((SES_add-e_test[,"power"])^2))))
print(paste0("RMSE (HW multi): ",sqrt(mean((SES_m-e_test[,"power"])^2))))
print(paste0("RMSE (Arima): ",sqrt(mean((ar_f$mean-e_test[,"power"])^2))))
print(paste0("RMSE (Auto Arima): ",sqrt(mean((aa_f$mean-e_test[,"power"])^2))))
print(paste0("RMSE (Auto Arima deep): ",sqrt(mean((aa2_f$mean-e_test[,"power"])^2))))
print(paste0("RMSE (Sarima): ",sqrt(mean((sarima_f$mean-e_test[,"power"])^2))))
print(paste0("RMSE (Neural Network): ",sqrt(mean((nn_f$mean-e_test[,"power"])^2))))

```

Based on the results, SES seems to outperform the other models. Going to be using this for predicting values.


## Forecasting with Temperature

We want to be able to predict Power using other time series models. We add seasonality and trend into the mix.

```{r}
lm=tslm(power~temp+season+trend,data=e_train)
summary(lm)
```

We can say that temperature is a significant feature, given that trend and majority of the seasons are below 0.05. So it needs to be added to the forecast.

```{r}
checkresiduals(lm)
pacf(lm$residuals)
```

We fit the best Arima model with temperature.

```{r}
s_2 = Arima(e_train[,"power"],xreg=e_train[,"temp"],order=c(5,0,0),seasonal = c(0,1,0))
checkresiduals((s_2))
```
```{r}
nn_2=nnetar(e_train[,"power"],xreg=e_train[,"temp"])
checkresiduals(nn_2)
```

How about we test a Vectorial Auto Regressive Model?

Grouped Time Series models. Let's go! We take both columns.

```{r}
e_var <- e_train
e_var <- e_var[, c("power","temp")]
```
```{r}
library(vars)
VARselect(e_var, lag.max=10, type="const")
```

AIC leads me to select an order equal to 10.


```{r}
var <- VAR(e_var, p=10, type="const")
summary(var)
```
```{r}
serial.test(var, lags.pt=10, type="PT.asymptotic")
```

Forecast

```{r}
library(forecast)
s2_f <- forecast(s_2,xreg=e_test[,"temp"],96)
nn2_f <- forecast(nn_2, xreg=e_test[,"temp"],96)
v_f <- forecast(var, xreg=e_test[,"temp"], h=96)

print(paste0("RMSE (Sarima): ",sqrt(mean((s2_f$mean-e_test[,"power"])^2))))
print(paste0("RMSE (NN): ",sqrt(mean((nn2_f$mean-e_test[,"power"])^2))))
print(paste0("RMSE (Var - Power): ",sqrt(mean((v_f$forecast$power$mean-e_test[,"power"])^2))))
print(paste0("RMSE (Var - Temp): ",sqrt(mean((v_f$forecast$temp$mean-e_test[,"temp"])^2))))

```

Testing VAR here wouldn't work since we only have two columns and as much as Var - Temp has 3.39 RMSE, that's not what i want to use or predict so here we give Neural Networks (at 17.7032301905825) - the nod for Forecasting with Temperature.

Preparing models for saving.

```{r}
# SES model to predict without temperature.

SES_pred=ses(na.omit(elec[,"power"],alpha=NULL,beta=NULL,gamma=NULL))
ses_pred_f=forecast(SES_pred,xreg=pred_temp, n.ahead=96)

SES_pred_multi=HoltWinters(na.omit(elec[,"power"],alpha=NULL,beta=NULL,gamma=NULL,seasonal = 'multi' ))
SES_pred_multi_f<-predict(SES_pred_multi, n.ahead=96)

NN_pred= nnetar(na.omit(elec[,"power"]), xreg=elec[1:nrow(values_of_e),"temp"])
nn_pred_f=forecast(NN_pred,xreg=pred_temp, h=96)


```

## Predictions, Save CSV.

Initially, SES was my forecasting pick without temperature.

But in the end, the observations of SES weren't enough to predict the next day as it only generated values for 10 observations when we need 96. 

This means I have to choose the next lowest predictor and that was Holt Winters Multi at 13.9237614756714.

As for the forecasts with temperature, Neural Networks wins.

Now we write to CSV.




```{r}
library(xts)

# sp <- as.data.frame(ses_pred_f$mean[1:96])
sm <- as.data.frame(SES_pred_multi_f[1:96])
np <- as.data.frame(nn_pred_f$mean[1:96])                    

names(sm) <- c("Prediction without Temperature")
names(np) <- c("Prediction with Temperature")

new <- sm
new <- cbind(sm, np)
new

# we save our two columns in one file.
write.xlsx(new, "Predict.xls", row.names=FALSE, append=TRUE)

```




